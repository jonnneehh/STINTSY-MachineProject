{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook file must be run after taking the output from datasetPreparation.ipynb\n",
    "\n",
    "Training of the model will be done ONLY. Manipulation of the dataset must NOT be done here. Only importing of dataset is allowed. \n",
    "\n",
    "Train Test Splits will be done on datasetPrepation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will output the top performing ZERO-SHOT models. No hyperparameter tuning will be done in this notebook.\n",
    "\n",
    "Technique used will be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing of dataset: train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (39818, 6)\n",
      "X_test shape: (17065, 6)\n",
      "y_train shape: (39818, 1)\n",
      "y_test shape: (17065, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('data/cleaned/X_train.csv')\n",
    "X_test = pd.read_csv('data/cleaned/X_test.csv')\n",
    "y_train = pd.read_csv('data/cleaned/y_train.csv')\n",
    "y_test = pd.read_csv('data/cleaned/y_test.csv')\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "zero_shot_performances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape y_train and y_test to be 1D arrays\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **UNCOMMENT TO IMPORT ORIGINAL, CLEANED, AND UNDERSAMPLED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_X_train = pd.read_csv('data/original/X_train.csv')\n",
    "# cleaned_X_train = pd.read_csv('data/cleaned/X_train.csv')\n",
    "# undersampled_X_train = pd.read_csv('data/undersampled/X_train.csv')\n",
    "\n",
    "# original_X_test = pd.read_csv('data/original/X_test.csv')\n",
    "# cleaned_X_test = pd.read_csv('data/cleaned/X_test.csv')\n",
    "# undersampled_X_test = pd.read_csv('data/undersampled/X_test.csv')\n",
    "\n",
    "# original_y_train = pd.read_csv('data/original/y_train.csv')\n",
    "# cleaned_y_train = pd.read_csv('data/cleaned/y_train.csv')\n",
    "# undersampled_y_train = pd.read_csv('data/undersampled/y_train.csv')\n",
    "\n",
    "# original_y_test = pd.read_csv('data/original/y_test.csv')\n",
    "# cleaned_y_test = pd.read_csv('data/cleaned/y_test.csv')\n",
    "# undersampled_y_test = pd.read_csv('data/undersampled/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_report(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename, X_test, y_test):\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Model Found: Loading...\")\n",
    "        model = joblib.load(filename)\n",
    "        performance = get_performance_report(model, X_test, y_test)\n",
    "        print(performance)\n",
    "\n",
    "        return model, performance, True\n",
    "    \n",
    "    else:\n",
    "        print(f\"Model {filename} not found\")\n",
    "\n",
    "        return None, None, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(report):\n",
    "    # Define the headers\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    \n",
    "    # Print the header\n",
    "    print(f\"{'':>15} {'precision':>10} {'recall':>10} {'f1-score':>10} {'support':>10}\")\n",
    "    \n",
    "    # Print the per-class metrics\n",
    "    for class_name, metrics in report.items():\n",
    "        if class_name in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            continue  # Skip overall metrics for now\n",
    "        print(f\"{class_name:>15} {metrics['precision']:>10.2f} {metrics['recall']:>10.2f} {metrics['f1-score']:>10.2f} {metrics['support']:>10}\")\n",
    "    \n",
    "    # Print the overall accuracy\n",
    "    print(f\"\\n{'accuracy':>15} {' ':>10} {' ':>10} {report['accuracy']:>10.2f} {sum([report[class_name]['support'] for class_name in report if class_name not in ['accuracy', 'macro avg', 'weighted avg']]):>10}\")\n",
    "\n",
    "    # Print the macro and weighted averages\n",
    "    for avg in ['macro avg', 'weighted avg']:\n",
    "        metrics = report[avg]\n",
    "        print(f\"{avg:>15} {metrics['precision']:>10.2f} {metrics['recall']:>10.2f} {metrics['f1-score']:>10.2f} {metrics['support']:>10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following models will be used for training on a stars classification dataset:\n",
    "\n",
    "1. **Logistic Regression**: A linear model for binary classification, extended here for multi-class classification. It models the probability of each class using the logistic function.\n",
    "\n",
    "2. **Support Vector Machines (SVM)**: A classification model that finds the hyperplane best separating the data into classes, effective for high-dimensional data.\n",
    "\n",
    "3. **Gaussian Naive Bayes**: A probabilistic classifier based on Bayes' theorem with the assumption of feature independence. Suitable for high-dimensional datasets.\n",
    "\n",
    "4. **K-Nearest Neighbors (KNN)**: An instance-based learning algorithm that classifies a sample based on the majority class among its k-nearest neighbors, effective for non-linear decision boundaries.\n",
    "\n",
    "5. **Decision Trees**: A model that splits the data into subsets based on feature values, providing a clear visual representation of decision-making, suitable for both classification and regression.\n",
    "\n",
    "6. **Extreme Gradient Boost (XGBoost)**: An efficient implementation of gradient-boosted decision trees designed for speed and performance, widely used for structured/tabular data.\n",
    "\n",
    "7. **Light Gradient Boost Machine (LightGBM)**: A highly efficient gradient boosting framework optimized for speed and memory usage, making it suitable for large datasets and high-dimensional data.\n",
    "\n",
    "8. **Random Forest:** An ensemble method that uses multiple decision trees to improve classification accuracy and control overfitting. It builds numerous decision trees during training and outputs the mode of the classes for classification tasks.\n",
    "\n",
    "9. **AdaBoost:** An ensemble method that combines multiple weak classifiers to form a strong classifier. It adjusts the weights of incorrectly classified instances so that subsequent classifiers focus more on difficult cases, improving overall accuracy.\n",
    "\n",
    "10. **Feedforward Neural Network (FNN)**: A simple type of artificial neural network where information moves in one direction, from input to output, suitable for basic and complex classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_zero_shot** gets the model and trains it one time only.\n",
    "\n",
    "Returns: classification report of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_zero_shot(model, X_train, X_test, y_train, y_test):\n",
    "    # Fit model once\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict X_test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Get classification report\n",
    "    class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    return class_report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_tuning(model, param_grid, X_train, X_test, y_train, y_test):\n",
    "    # Perform hyperparameter tuning with GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=5, verbose=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Train the best model on the training set\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    return class_report, best_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision     recall   f1-score    support\n",
      "              0       0.93       0.95       0.94       5688\n",
      "              1       0.95       0.93       0.94       5688\n",
      "              2       0.99       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.96      17065\n",
      "      macro avg       0.96       0.96       0.96      17065\n",
      "   weighted avg       0.96       0.96       0.96      17065\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "report = train_zero_shot(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "zero_shot_performances.append(report)\n",
    "\n",
    "print_classification_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision     recall   f1-score    support\n",
      "              0       0.94       0.97       0.95       5688\n",
      "              1       0.98       0.93       0.95       5688\n",
      "              2       0.99       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n"
     ]
    }
   ],
   "source": [
    "report = train_zero_shot(SVC(), X_train, X_test, y_train, y_test)\n",
    "\n",
    "zero_shot_performances.append(report)\n",
    "\n",
    "print_classification_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision     recall   f1-score    support\n",
      "              0       0.87       0.76       0.81       5688\n",
      "              1       0.80       0.89       0.84       5688\n",
      "              2       0.99       0.99       0.99       5689\n",
      "\n",
      "       accuracy                             0.88      17065\n",
      "      macro avg       0.89       0.88       0.88      17065\n",
      "   weighted avg       0.89       0.88       0.88      17065\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "report = train_zero_shot(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "zero_shot_performances.append(report)\n",
    "\n",
    "print_classification_report(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define parameters\n",
    "# param_grid = {\n",
    "#     'var_smoothing': np.logspace(0, -9, num=100)\n",
    "# }\n",
    "\n",
    "# filename = \"models/gnb_model.joblib\"\n",
    "\n",
    "# gnb_model, gnb_performance, isLoaded = load_model(filename, X_test, y_test)\n",
    "\n",
    "# # Train Model\n",
    "# if not(isLoaded):\n",
    "#     print(\"Training GNB model\")\n",
    "#     gnb_model, gnb_performance = train_with_tuning(param_grid, X_train, X_test, y_train, y_test)\n",
    "\n",
    "#     # Export model\n",
    "#     joblib.dump(gnb_model, filename)\n",
    "\n",
    "# # Put model in array\n",
    "# models.append(gnb_model)\n",
    "# model_performances.append(gnb_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision     recall   f1-score    support\n",
      "              0       0.94       0.97       0.95       5688\n",
      "              1       0.97       0.94       0.96       5688\n",
      "              2       0.99       1.00       0.99       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "\n",
    "report = train_zero_shot(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "zero_shot_performances.append(report)\n",
    "\n",
    "print_classification_report(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'n_neighbors': [3, 5, 7, 9, 11],\n",
    "#     'weights': ['uniform', 'distance'],\n",
    "#     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "#     'p': [1, 2]\n",
    "# }\n",
    "\n",
    "# filename = \"models/knn_model.joblib\"\n",
    "\n",
    "# knn_model, knn_performance, isLoaded = load_model(filename, X_test, y_test)\n",
    "\n",
    "# # Train Model\n",
    "# if not(isLoaded):\n",
    "#     print(\"Training KNN model\")\n",
    "#     knn_model, knn_performance = k_nearest_neighbors(param_grid, X_train, X_test, y_train, y_test)\n",
    "\n",
    "#     # Export model\n",
    "#     joblib.dump(knn_model, filename)\n",
    "\n",
    "# # Put model in array\n",
    "# models.append(knn_model)\n",
    "# model_performances.append(knn_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision     recall   f1-score    support\n",
      "              0       0.94       0.94       0.94       5688\n",
      "              1       0.94       0.94       0.94       5688\n",
      "              2       1.00       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.96      17065\n",
      "      macro avg       0.96       0.96       0.96      17065\n",
      "   weighted avg       0.96       0.96       0.96      17065\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "\n",
    "report = train_zero_shot(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "zero_shot_performances.append(report)\n",
    "\n",
    "print_classification_report(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid for hyperparameter tuning\n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'splitter': ['best', 'random'],\n",
    "#     'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'max_features': [None, 'sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "# filename = \"models/dt_model.joblib\"\n",
    "\n",
    "# dt_model, dt_performance, isLoaded = load_model(filename, X_test, y_test)\n",
    "\n",
    "# # Train Model\n",
    "# if not(isLoaded):\n",
    "#     print(\"Training DT model\")\n",
    "#     dt_model, dt_performance = decision_tree(param_grid, X_train, X_test, y_train, y_test)\n",
    "\n",
    "#     # Export model\n",
    "#     joblib.dump(dt_model, filename)\n",
    "\n",
    "# # Put model in array\n",
    "# models.append(dt_model)\n",
    "# model_performances.append(dt_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision     recall   f1-score    support\n",
      "              0       0.95       0.97       0.96       5688\n",
      "              1       0.97       0.95       0.96       5688\n",
      "              2       1.00       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "report = train_zero_shot(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "zero_shot_performances.append(report)\n",
    "\n",
    "print_classification_report(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB Model param_grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.01, 0.1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 39818, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score -1.098587\n",
      "[LightGBM] [Info] Start training from score -1.098587\n",
      "[LightGBM] [Info] Start training from score -1.098663\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.95       0.97       0.96       5688\n",
      "              1       0.97       0.95       0.96       5688\n",
      "              2       0.99       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "report = train_zero_shot(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "zero_shot_performances.append(report)\n",
    "\n",
    "print_classification_report(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM Model param_grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [-1, 10, 20, 30],\n",
    "    'min_data_in_leaf': [20, 50, 100],\n",
    "    'feature_fraction': [0.6, 0.8, 1.0],\n",
    "    'bagging_fraction': [0.6, 0.8, 1.0],\n",
    "    'bagging_freq': [0, 5, 10],\n",
    "    'lambda_l1': [0, 0.01, 0.1],\n",
    "    'lambda_l2': [0, 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision     recall   f1-score    support\n",
      "              0       0.95       0.97       0.96       5688\n",
      "              1       0.98       0.95       0.96       5688\n",
      "              2       1.00       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "\n",
    "report = train_zero_shot(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "zero_shot_performances.append(report)\n",
    "\n",
    "print_classification_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision     recall   f1-score    support\n",
      "              0       0.46       0.81       0.59       5688\n",
      "              1       0.33       0.09       0.14       5688\n",
      "              2       0.99       0.97       0.98       5689\n",
      "\n",
      "       accuracy                             0.62      17065\n",
      "      macro avg       0.59       0.62       0.57      17065\n",
      "   weighted avg       0.59       0.62       0.57      17065\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier()\n",
    "\n",
    "report = train_zero_shot(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "zero_shot_performances.append(report)\n",
    "\n",
    "print_classification_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X_train, X_test, y_train, y_test):\n",
    "    # Determine the number of classes\n",
    "    num_classes = 3\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    \n",
    "    # Define the neural network model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train_one_hot, epochs=1, batch_size=32, validation_split=0.1)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = y_pred_probs.argmax(axis=1)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 1s 778us/step - loss: 0.1865 - accuracy: 0.9415 - val_loss: 0.1067 - val_accuracy: 0.9661\n",
      "534/534 [==============================] - 0s 548us/step\n",
      "Model Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "report = train_neural_network(X_train, X_test, y_train, y_test)\n",
    "\n",
    "zero_shot_performances.append(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare all zero shot performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.93       0.95       0.94       5688\n",
      "              1       0.95       0.93       0.94       5688\n",
      "              2       0.99       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.96      17065\n",
      "      macro avg       0.96       0.96       0.96      17065\n",
      "   weighted avg       0.96       0.96       0.96      17065\n",
      "-----------------------------------------------------------\n",
      "Support Vector Machines\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.94       0.97       0.95       5688\n",
      "              1       0.98       0.93       0.95       5688\n",
      "              2       0.99       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n",
      "-----------------------------------------------------------\n",
      "Gaussian Naive Bayes\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.87       0.76       0.81       5688\n",
      "              1       0.80       0.89       0.84       5688\n",
      "              2       0.99       0.99       0.99       5689\n",
      "\n",
      "       accuracy                             0.88      17065\n",
      "      macro avg       0.89       0.88       0.88      17065\n",
      "   weighted avg       0.89       0.88       0.88      17065\n",
      "-----------------------------------------------------------\n",
      "K-Nearest Neighbors\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.94       0.97       0.95       5688\n",
      "              1       0.97       0.94       0.96       5688\n",
      "              2       0.99       1.00       0.99       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n",
      "-----------------------------------------------------------\n",
      "Decision Trees\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.94       0.94       0.94       5688\n",
      "              1       0.94       0.94       0.94       5688\n",
      "              2       1.00       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.96      17065\n",
      "      macro avg       0.96       0.96       0.96      17065\n",
      "   weighted avg       0.96       0.96       0.96      17065\n",
      "-----------------------------------------------------------\n",
      "Extreme Gradient Boost\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.95       0.97       0.96       5688\n",
      "              1       0.97       0.95       0.96       5688\n",
      "              2       1.00       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n",
      "-----------------------------------------------------------\n",
      "Light Gradient Boost Machine\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.95       0.97       0.96       5688\n",
      "              1       0.97       0.95       0.96       5688\n",
      "              2       0.99       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n",
      "-----------------------------------------------------------\n",
      "Random Forest\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.95       0.97       0.96       5688\n",
      "              1       0.98       0.95       0.96       5688\n",
      "              2       1.00       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n",
      "-----------------------------------------------------------\n",
      "AdaBoost\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.46       0.81       0.59       5688\n",
      "              1       0.33       0.09       0.14       5688\n",
      "              2       0.99       0.97       0.98       5689\n",
      "\n",
      "       accuracy                             0.62      17065\n",
      "      macro avg       0.59       0.62       0.57      17065\n",
      "   weighted avg       0.59       0.62       0.57      17065\n",
      "-----------------------------------------------------------\n",
      "Feedforward Neural Network\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.93       0.96       0.95       5688\n",
      "              1       0.97       0.93       0.95       5688\n",
      "              2       0.99       1.00       0.99       5689\n",
      "\n",
      "       accuracy                             0.96      17065\n",
      "      macro avg       0.96       0.96       0.96      17065\n",
      "   weighted avg       0.96       0.96       0.96      17065\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_titles = [\n",
    "    'Logistic Regression',\n",
    "    'Support Vector Machines',\n",
    "    'Gaussian Naive Bayes',\n",
    "    'K-Nearest Neighbors',\n",
    "    'Decision Trees',\n",
    "    'Extreme Gradient Boost',\n",
    "    'Light Gradient Boost Machine',\n",
    "    'Random Forest',\n",
    "    'AdaBoost',\n",
    "    'Feedforward Neural Network'\n",
    "]\n",
    "\n",
    "for title, report in zip(model_titles, zero_shot_performances):\n",
    "    print(title)\n",
    "    print_classification_report(report)\n",
    "    print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.973630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Extreme Gradient Boost</td>\n",
       "      <td>0.973513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Light Gradient Boost Machine</td>\n",
       "      <td>0.972986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.968649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.967184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Feedforward Neural Network</td>\n",
       "      <td>0.963551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Trees</td>\n",
       "      <td>0.959742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.959156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.883211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.622678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy\n",
       "7                 Random Forest  0.973630\n",
       "5        Extreme Gradient Boost  0.973513\n",
       "6  Light Gradient Boost Machine  0.972986\n",
       "3           K-Nearest Neighbors  0.968649\n",
       "1       Support Vector Machines  0.967184\n",
       "9    Feedforward Neural Network  0.963551\n",
       "4                Decision Trees  0.959742\n",
       "0           Logistic Regression  0.959156\n",
       "2          Gaussian Naive Bayes  0.883211\n",
       "8                      AdaBoost  0.622678"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracies = pd.DataFrame(columns=('Model', 'Accuracy'))\n",
    "\n",
    "for title, report in zip(model_titles, zero_shot_performances):\n",
    "    model_accuracies = pd.concat([model_accuracies, pd.DataFrame({'Model': [title], 'Accuracy': [report['accuracy']]})], ignore_index=True)\n",
    "\n",
    "\n",
    "model_accuracies.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get the top five models and tune each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models_zero_shot = [RandomForestClassifier(), XGBClassifier(), lgb.LGBMClassifier(), KNeighborsClassifier(), SVC()]\n",
    "top_model_titles = [\n",
    "    'Random Forest',\n",
    "    'Extreme Gradient Boost',\n",
    "    'Light Gradient Boost Machine',\n",
    "    'K-Nearest Neighbors',\n",
    "    'Support Vector Machines',\n",
    "]\n",
    "\n",
    "hyper_parameters = [\n",
    "    {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.8, 1.0]\n",
    "    },\n",
    "    {\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300]\n",
    "    },\n",
    "    {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "]\n",
    "\n",
    "models_after_hyperparameter_tuning = []\n",
    "\n",
    "tuned_model_data = pd.DataFrame(columns=['Model', 'Accuracy', \"Best Hyperparameters\", \"Time To Train\"])\n",
    "\n",
    "reports_after_tuning = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Model Accuracy: 0.97\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9506003430531732, 'recall': 0.9743319268635724, 'f1-score': 0.9623198471956937, 'support': 5688}, '1': {'precision': 0.9761388286334056, 'recall': 0.9493670886075949, 'f1-score': 0.9625668449197861, 'support': 5688}, '2': {'precision': 0.9975451516745573, 'recall': 1.0, 'f1-score': 0.9987710674157303, 'support': 5689}, 'accuracy': 0.97456782888954, 'macro avg': {'precision': 0.9747614411203788, 'recall': 0.9745663384903892, 'f1-score': 0.9745525865104034, 'support': 17065}, 'weighted avg': {'precision': 0.9747627762338013, 'recall': 0.97456782888954, 'f1-score': 0.9745540057006117, 'support': 17065}}\n",
      "Total time taken to train Random Forest: 697.62 seconds\n",
      "Best Hyperparameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.95       0.97       0.96       5688\n",
      "              1       0.98       0.95       0.96       5688\n",
      "              2       1.00       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n",
      "\n",
      "----------\n",
      "\n",
      "Training Extreme Gradient Boost\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Model Accuracy: 0.97\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9506003430531732, 'recall': 0.9743319268635724, 'f1-score': 0.9623198471956937, 'support': 5688}, '1': {'precision': 0.976878612716763, 'recall': 0.9507735583684951, 'f1-score': 0.9636493228795439, 'support': 5688}, '2': {'precision': 0.9966660817687314, 'recall': 0.9984179996484444, 'f1-score': 0.9975412715138743, 'support': 5689}, 'accuracy': 0.9745092294169353, 'macro avg': {'precision': 0.9747150125128892, 'recall': 0.974507828293504, 'f1-score': 0.974503480529704, 'support': 17065}, 'weighted avg': {'precision': 0.9747162988339706, 'recall': 0.9745092294169353, 'f1-score': 0.9745048305321057, 'support': 17065}}\n",
      "Total time taken to train Extreme Gradient Boost: 117.40 seconds\n",
      "Best Hyperparameters: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.95       0.97       0.96       5688\n",
      "              1       0.98       0.95       0.96       5688\n",
      "              2       1.00       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n",
      "\n",
      "----------\n",
      "\n",
      "Training Light Gradient Boost Machine\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 39818, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score -1.098587\n",
      "[LightGBM] [Info] Start training from score -1.098587\n",
      "[LightGBM] [Info] Start training from score -1.098663\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 39818, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score -1.098587\n",
      "[LightGBM] [Info] Start training from score -1.098587\n",
      "[LightGBM] [Info] Start training from score -1.098663\n",
      "Model Accuracy: 0.97\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9517968210089841, 'recall': 0.9685302390998594, 'f1-score': 0.9600906239107703, 'support': 5688}, '1': {'precision': 0.9733956498292289, 'recall': 0.9520042194092827, 'f1-score': 0.9625811039018755, 'support': 5688}, '2': {'precision': 0.9942247112355618, 'recall': 0.9985937774652839, 'f1-score': 0.9964044549679909, 'support': 5689}, 'accuracy': 0.9730442426018165, 'macro avg': {'precision': 0.9731390606912583, 'recall': 0.9730427453248086, 'f1-score': 0.9730253942602122, 'support': 17065}, 'weighted avg': {'precision': 0.9731402962992598, 'recall': 0.9730442426018165, 'f1-score': 0.9730267642608397, 'support': 17065}}\n",
      "Total time taken to train Light Gradient Boost Machine: 60.91 seconds\n",
      "Best Hyperparameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.95       0.97       0.96       5688\n",
      "              1       0.97       0.95       0.96       5688\n",
      "              2       0.99       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n",
      "\n",
      "----------\n",
      "\n",
      "Training K-Nearest Neighbors\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Model Accuracy: 0.97\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9420140675930692, 'recall': 0.965365682137834, 'f1-score': 0.9535469306242945, 'support': 5688}, '1': {'precision': 0.9738134206219312, 'recall': 0.9414556962025317, 'f1-score': 0.9573612228479484, 'support': 5688}, '2': {'precision': 0.9902388007669514, 'recall': 0.9985937774652839, 'f1-score': 0.9943987397164362, 'support': 5689}, 'accuracy': 0.9684734837386464, 'macro avg': {'precision': 0.9686887629939839, 'recall': 0.9684717186018831, 'f1-score': 0.968435631062893, 'support': 17065}, 'weighted avg': {'precision': 0.968690025814832, 'recall': 0.9684734837386464, 'f1-score': 0.9684371524873673, 'support': 17065}}\n",
      "Total time taken to train K-Nearest Neighbors: 3.20 seconds\n",
      "Best Hyperparameters: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.94       0.97       0.95       5688\n",
      "              1       0.97       0.94       0.96       5688\n",
      "              2       0.99       1.00       0.99       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n",
      "\n",
      "----------\n",
      "\n",
      "Training Support Vector Machines\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Model Accuracy: 0.97\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9383934649421375, 'recall': 0.9694092827004219, 'f1-score': 0.9536492563126945, 'support': 5688}, '1': {'precision': 0.9779573842762674, 'recall': 0.9360056258790436, 'f1-score': 0.9565217391304347, 'support': 5688}, '2': {'precision': 0.9902523933855527, 'recall': 1.0, 'f1-score': 0.9951023263949624, 'support': 5689}, 'accuracy': 0.9684734837386464, 'macro avg': {'precision': 0.9688677475346527, 'recall': 0.9684716361931551, 'f1-score': 0.9684244406126972, 'support': 17065}, 'weighted avg': {'precision': 0.968869000663621, 'recall': 0.9684734837386464, 'f1-score': 0.9684260039227341, 'support': 17065}}\n",
      "Total time taken to train Support Vector Machines: 76.19 seconds\n",
      "Best Hyperparameters: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "                 precision     recall   f1-score    support\n",
      "              0       0.94       0.97       0.95       5688\n",
      "              1       0.98       0.94       0.96       5688\n",
      "              2       0.99       1.00       1.00       5689\n",
      "\n",
      "       accuracy                             0.97      17065\n",
      "      macro avg       0.97       0.97       0.97      17065\n",
      "   weighted avg       0.97       0.97       0.97      17065\n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for title, model, param_grid in zip(top_model_titles, top_models_zero_shot, hyper_parameters):\n",
    "    print(f\"Training {title}\")\n",
    "\n",
    "    # Get start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Start hyperparameter tuning\n",
    "    report, best_model = train_with_tuning(model, param_grid, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Get end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Get elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total time taken to train {title}: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Get best hyperparameters of the model\n",
    "    hyperparameters = model.get_params()\n",
    "    print(f\"Best Hyperparameters: {hyperparameters}\")\n",
    "\n",
    "    # Append report data\n",
    "    reports_after_tuning.append(report)\n",
    "    print_classification_report(report)\n",
    "\n",
    "    # Append Dataframe data\n",
    "    tuned_model_data = pd.concat([tuned_model_data, pd.DataFrame({'Model': [title], \n",
    "                                                                  'Accuracy': [report['accuracy']], \n",
    "                                                                  \"Best Hyperparameters\": [hyperparameters], \n",
    "                                                                  \"Time To Train\": [elapsed_time]\n",
    "                                                                  })], ignore_index=True)\n",
    "\n",
    "    # Save models just in case\n",
    "    models_after_hyperparameter_tuning.append(best_model)\n",
    "    \n",
    "    print(\"\\n----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Best Hyperparameters</th>\n",
       "      <th>Time To Train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.974568</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...</td>\n",
       "      <td>697.622665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extreme Gradient Boost</td>\n",
       "      <td>0.974509</td>\n",
       "      <td>{'objective': 'binary:logistic', 'base_score':...</td>\n",
       "      <td>117.401529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Light Gradient Boost Machine</td>\n",
       "      <td>0.973044</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>60.905026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.968473</td>\n",
       "      <td>{'algorithm': 'auto', 'leaf_size': 30, 'metric...</td>\n",
       "      <td>3.198234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.968473</td>\n",
       "      <td>{'C': 1.0, 'break_ties': False, 'cache_size': ...</td>\n",
       "      <td>76.192002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy  \\\n",
       "0                 Random Forest  0.974568   \n",
       "1        Extreme Gradient Boost  0.974509   \n",
       "2  Light Gradient Boost Machine  0.973044   \n",
       "3           K-Nearest Neighbors  0.968473   \n",
       "4       Support Vector Machines  0.968473   \n",
       "\n",
       "                                Best Hyperparameters  Time To Train  \n",
       "0  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...     697.622665  \n",
       "1  {'objective': 'binary:logistic', 'base_score':...     117.401529  \n",
       "2  {'boosting_type': 'gbdt', 'class_weight': None...      60.905026  \n",
       "3  {'algorithm': 'auto', 'leaf_size': 30, 'metric...       3.198234  \n",
       "4  {'C': 1.0, 'break_ties': False, 'cache_size': ...      76.192002  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
