{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook file must be run after taking the output from datasetPreparation.ipynb\n",
    "\n",
    "Training of the model will be done ONLY. Manipulation of the dataset must NOT be done here. Only importing of dataset is allowed. \n",
    "\n",
    "Train Test Splits will be done on datasetPrepation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will output the top performing ZERO-SHOT models. No hyperparameter tuning will be done in this notebook.\n",
    "\n",
    "Technique used will be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing of dataset: train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (69999, 6)\n",
      "X_test shape: (30000, 6)\n",
      "y_train shape: (69999, 1)\n",
      "y_test shape: (30000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('data/cleaned/X_train.csv')\n",
    "X_test = pd.read_csv('data/cleaned/X_test.csv')\n",
    "y_train = pd.read_csv('data/cleaned/y_train.csv')\n",
    "y_test = pd.read_csv('data/cleaned/y_test.csv')\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "model_performances = []\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_report(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename, X_test, y_test):\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Model Found: Loading...\")\n",
    "        model = joblib.load(filename)\n",
    "        performance = get_performance_report(model, X_test, y_test)\n",
    "        print(performance)\n",
    "\n",
    "        return model, performance, True\n",
    "    \n",
    "    else:\n",
    "        print(f\"Model {filename} not found\")\n",
    "\n",
    "        return None, None, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_naive_bayes(param_grid, X_train, X_test, y_train, y_test):\n",
    "    # Reshape y_train and y_test to be 1D arrays\n",
    "    y_train = np.ravel(y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "\n",
    "    # Initialize the Gaussian Naive Bayes model\n",
    "    gnb = GaussianNB()\n",
    "    \n",
    "    # Perform hyperparameter tuning with GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=gnb, param_grid=param_grid, scoring='accuracy', cv=5, verbose=3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_gnb = grid_search.best_estimator_\n",
    "    \n",
    "    # Train the best model on the training set\n",
    "    best_gnb.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_gnb.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    return best_gnb, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Found: Loading...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      GALAXY       0.96      0.91      0.93     17834\n",
      "         QSO       0.78      0.88      0.82      5688\n",
      "        STAR       0.98      1.00      0.99      6478\n",
      "\n",
      "    accuracy                           0.92     30000\n",
      "   macro avg       0.90      0.93      0.92     30000\n",
      "weighted avg       0.93      0.92      0.92     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "param_grid = {\n",
    "    'var_smoothing': np.logspace(0, -9, num=100)\n",
    "}\n",
    "\n",
    "filename = \"models/gnb_model.joblib\"\n",
    "\n",
    "gnb_model, gnb_performance, isLoaded = load_model(filename, X_test, y_test)\n",
    "\n",
    "# Train Model\n",
    "if not(isLoaded):\n",
    "    print(\"Training GNB model\")\n",
    "    gnb_model, gnb_performance = gaussian_naive_bayes(param_grid, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Export model\n",
    "    joblib.dump(gnb_model, filename)\n",
    "\n",
    "# Put model in array\n",
    "models.append(gnb_model)\n",
    "model_performances.append(gnb_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(param_grid, X_train, X_test, y_train, y_test):\n",
    "    # Reshape y_train and y_test to be 1D arrays if necessary\n",
    "    y_train = np.ravel(y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    \n",
    "    # Initialize the K-Nearest Neighbors model\n",
    "    knn = KNeighborsClassifier()\n",
    "    \n",
    "    # Perform hyperparameter tuning with GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, scoring='accuracy', cv=5, verbose=3, n_jobs = -1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_knn = grid_search.best_estimator_\n",
    "    \n",
    "    # Train the best model on the training set\n",
    "    best_knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_knn.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    return best_knn, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Found: Loading...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      GALAXY       0.96      0.96      0.96     17834\n",
      "         QSO       0.95      0.91      0.93      5688\n",
      "        STAR       0.93      0.96      0.94      6478\n",
      "\n",
      "    accuracy                           0.95     30000\n",
      "   macro avg       0.95      0.94      0.95     30000\n",
      "weighted avg       0.95      0.95      0.95     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "filename = \"models/knn_model.joblib\"\n",
    "\n",
    "knn_model, knn_performance, isLoaded = load_model(filename, X_test, y_test)\n",
    "\n",
    "# Train Model\n",
    "if not(isLoaded):\n",
    "    print(\"Training KNN model\")\n",
    "    knn_model, knn_performance = k_nearest_neighbors(param_grid, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Export model\n",
    "    joblib.dump(knn_model, filename)\n",
    "\n",
    "# Put model in array\n",
    "models.append(knn_model)\n",
    "model_performances.append(knn_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(param_grid, X_train, X_test, y_train, y_test):\n",
    "    # Reshape y_train and y_test to be 1D arrays if necessary\n",
    "    y_train = np.ravel(y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    \n",
    "    # Initialize the Decision Tree Classifier model\n",
    "    dt = DecisionTreeClassifier()\n",
    "    \n",
    "    # Perform hyperparameter tuning with GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, scoring='accuracy', cv=5, verbose = 3, n_jobs = -1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_dt = grid_search.best_estimator_\n",
    "    \n",
    "    # Train the best model on the training set\n",
    "    best_dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_dt.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    return best_dt, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Found: Loading...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      GALAXY       0.97      0.98      0.98     17834\n",
      "         QSO       0.95      0.92      0.93      5688\n",
      "        STAR       1.00      1.00      1.00      6478\n",
      "\n",
      "    accuracy                           0.97     30000\n",
      "   macro avg       0.97      0.97      0.97     30000\n",
      "weighted avg       0.97      0.97      0.97     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "filename = \"models/dt_model.joblib\"\n",
    "\n",
    "dt_model, dt_performance, isLoaded = load_model(filename, X_test, y_test)\n",
    "\n",
    "# Train Model\n",
    "if not(isLoaded):\n",
    "    print(\"Training DT model\")\n",
    "    dt_model, dt_performance = decision_tree(param_grid, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Export model\n",
    "    joblib.dump(dt_model, filename)\n",
    "\n",
    "# Put model in array\n",
    "models.append(dt_model)\n",
    "model_performances.append(dt_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extreme_gradient_boosting(param_grid, X_train, X_test, y_train, y_test):\n",
    "    # Reshape y_train and y_test to be 1D arrays if necessary\n",
    "    y_train = np.ravel(y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    \n",
    "    # Initialize the XGBoost Classifier model\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "    \n",
    "    # Perform hyperparameter tuning with GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='accuracy', cv=5, verbose=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_xgb = grid_search.best_estimator_\n",
    "    \n",
    "    # Train the best model on the training set\n",
    "    best_xgb.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_xgb.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    return best_xgb, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model models/xgb_model.joblib not found\n",
      "Training DT model\n",
      "Fitting 5 folds for each of 6561 candidates, totalling 32805 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 32805 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n32805 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\sklearn.py\", line 1471, in fit\n    raise ValueError(\nValueError: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got ['GALAXY' 'QSO' 'STAR']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(isLoaded):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining DT model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m     xgb_model, xgb_performance \u001b[38;5;241m=\u001b[39m extreme_gradient_boosting(param_grid, X_train, X_test, y_train, y_test)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Export model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(xgb_model, filename)\n",
      "Cell \u001b[1;32mIn[62], line 11\u001b[0m, in \u001b[0;36mextreme_gradient_boosting\u001b[1;34m(param_grid, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Perform hyperparameter tuning with GridSearchCV\u001b[39;00m\n\u001b[0;32m     10\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mxgb, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Get the best model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m best_xgb \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_score)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 32805 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n32805 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\sklearn.py\", line 1471, in fit\n    raise ValueError(\nValueError: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got ['GALAXY' 'QSO' 'STAR']\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.01, 0.1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "filename = \"models/xgb_model.joblib\"\n",
    "\n",
    "xgb_model, xgb_performance, isLoaded = load_model(filename, X_test, y_test)\n",
    "\n",
    "# Train Model\n",
    "if not(isLoaded):\n",
    "    print(\"Training DT model\")\n",
    "    xgb_model, xgb_performance = extreme_gradient_boosting(param_grid, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Export model\n",
    "    joblib.dump(xgb_model, filename)\n",
    "\n",
    "# Put model in array\n",
    "models.append(xgb_model)\n",
    "model_performances.append(xgb_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 125505, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -0.693394\n",
      "[LightGBM] [Info] Start training from score -1.417870\n",
      "[LightGBM] [Info] Start training from score -1.355206\n",
      "LightGBM Model Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "def light_gradient_boosting_machine(param_grid, X_train, X_test, y_train, y_test):\n",
    "    # Reshape y_train and y_test to be 1D arrays if necessary\n",
    "    y_train = np.ravel(y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    \n",
    "    # Initialize the LightGBM Classifier model\n",
    "    lgbm = LGBMClassifier()\n",
    "    \n",
    "    # Perform hyperparameter tuning with GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring='accuracy', cv=5, verbose=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_lgbm = grid_search.best_estimator_\n",
    "    \n",
    "    # Train the best model on the training set\n",
    "    best_lgbm.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_lgbm.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    return best_lgbm, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [-1, 10, 20, 30],\n",
    "    'min_data_in_leaf': [20, 50, 100],\n",
    "    'feature_fraction': [0.6, 0.8, 1.0],\n",
    "    'bagging_fraction': [0.6, 0.8, 1.0],\n",
    "    'bagging_freq': [0, 5, 10],\n",
    "    'lambda_l1': [0, 0.01, 0.1],\n",
    "    'lambda_l2': [0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "filename = \"models/lgbm_model.joblib\"\n",
    "\n",
    "lgbm_model, lgbm_performance, isLoaded = load_model(filename, X_test, y_test)\n",
    "\n",
    "# Train Model\n",
    "if not(isLoaded):\n",
    "    print(\"Training DT model\")\n",
    "    lgbm_model, lgbm_performance = light_gradient_boosting_machine(param_grid, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Export model\n",
    "    joblib.dump(lgbm_model, filename)\n",
    "\n",
    "# Put model in array\n",
    "models.append(lgbm_model)\n",
    "model_performances.append(lgbm_performance)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
